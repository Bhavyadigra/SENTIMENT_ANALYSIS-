{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1q3l0nnMJoamno3Hlvc-WekvxcZxH2ka5","timestamp":1752177746779}],"authorship_tag":"ABX9TyMW2WnrnTly5FQgzOuTJToR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XMTjoqCj5NkW","executionInfo":{"status":"ok","timestamp":1752177686385,"user_tz":-330,"elapsed":273198,"user":{"displayName":"Bhavya Digra","userId":"05934256243314545940"}},"outputId":"0ab5f9da-8e2e-4dc3-e90a-70a3c56fd303"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["Original target values: [0 4]\n","\n","Class distribution:\n","target\n","0    800000\n","1    800000\n","Name: count, dtype: int64\n","\n","Final class labels: [0 1]\n","\n","Accuracy: 0.7677213231087616\n","Confusion Matrix:\n"," [[117809  42133]\n"," [ 32169 127772]]\n","Classification Report:\n","               precision    recall  f1-score   support\n","\n","           0       0.79      0.74      0.76    159942\n","           1       0.75      0.80      0.77    159941\n","\n","    accuracy                           0.77    319883\n","   macro avg       0.77      0.77      0.77    319883\n","weighted avg       0.77      0.77      0.77    319883\n","\n"]}],"source":["# === Importing Required Libraries ===\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Sklearn for modeling and evaluation\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n","\n","# NLTK for text preprocessing\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem.porter import PorterStemmer\n","import re\n","\n","# Download stopwords if not already downloaded\n","nltk.download('stopwords')\n","\n","# === Step 1: Load the Dataset ===\n","# Define column names explicitly since no header in CSV\n","columns = ['target', 'id', 'date', 'flag', 'user', 'text']\n","\n","# Load dataset from CSV\n","df = pd.read_csv('/content/review.csv', encoding='latin1', on_bad_lines='skip', names=columns)\n","\n","# Retain only the relevant columns\n","df = df[['text', 'target']]\n","\n","# Show unique classes before binarization\n","print(\"Original target values:\", df['target'].unique())\n","\n","# Convert target to binary: 0 = negative, 1 = positive\n","df['target'] = df['target'].replace(4, 1)\n","\n","# Display class distribution after replacement\n","print(\"\\nClass distribution:\")\n","print(df['target'].value_counts())\n","\n","# === Step 2: Preprocessing and Cleaning ===\n","# Initialize stopwords and stemmer\n","stop_words = set(stopwords.words('english'))\n","stemmer = PorterStemmer()\n","\n","# Define a function to clean and preprocess text\n","def clean_text(text):\n","    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n","    text = re.sub(r'[^a-zA-Z]', ' ', text)  # Keep only letters\n","    text = text.lower()  # Convert to lowercase\n","    text = text.split()  # Tokenize text\n","    # Remove stopwords and apply stemming\n","    text = [stemmer.stem(word) for word in text if word not in stop_words]\n","    return ' '.join(text)\n","\n","# Apply the cleaning function to all text data\n","df['cleaned_text'] = df['text'].apply(clean_text)\n","\n","# Optional: Remove rows with empty cleaned text\n","df = df[df['cleaned_text'].str.strip() != '']\n","\n","# === Step 3: Vectorization ===\n","# Convert text to numerical format using Bag-of-Words\n","vectorizer = CountVectorizer(max_features=5000)\n","x = vectorizer.fit_transform(df['cleaned_text'])  # keep it sparse!\n","\n","# Target variable\n","y = df['target']\n","\n","# Check number of classes to prevent training error\n","print(\"\\nFinal class labels:\", np.unique(y))\n","if len(np.unique(y)) < 2:\n","    raise ValueError(\"Target contains only one class. Cannot train model.\")\n","\n","# === Step 4: Train-Test Split ===\n","# Split data into training and testing sets (80/20)\n","X_train, X_test, y_train, y_test = train_test_split(\n","    x, y, test_size=0.2, random_state=42, stratify=y  # stratify to preserve class distribution\n",")\n","\n","# === Step 5: Model Training ===\n","# Initialize and train logistic regression model\n","model = LogisticRegression()\n","model.fit(X_train, y_train)\n","\n","# === Step 6: Model Evaluation ===\n","# Make predictions on the test set\n","y_pred = model.predict(X_test)\n","\n","# Evaluate model performance\n","print(\"\\nAccuracy:\", accuracy_score(y_test, y_pred))\n","print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n","print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n","\n","\n"]}]}